{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df25f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as tf\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e5a8c",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS W/O LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18523e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998083710670471},\n",
       " {'label': 'POSITIVE', 'score': 0.9810605049133301}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=pipeline(\"sentiment-analysis\")\n",
    "classifier([\"With great power comes great responsibility\",\"Failure is the part of success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5749cb",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=pipeline(\"zero-shot-classification\")\n",
    "classifier(\"With great power comes great responsibility\",candidate_labels=[\"education\",\"life\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49d25b",
   "metadata": {},
   "source": [
    "## TEXT-GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71dfaa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello Today I was writing a bit about the future of the web application called PHP 5.0, and I started on my way to writing another project'},\n",
       " {'generated_text': \"Hello Today I was inspired to become a fan of the first film (and possibly the most exciting movie I've ever watched). So I decided to make\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=pipeline(\"text-generation\",model='distilgpt2')\n",
    "classifier(\"Hello Today I was\",max_length=30,num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7436ce",
   "metadata": {},
   "source": [
    "## FILL MASK PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d17bae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.007368382532149553,\n",
       "  'token': 4110,\n",
       "  'token_str': ' Ali',\n",
       "  'sequence': \"Hello my name is Ali and I'm from India\"},\n",
       " {'score': 0.00667526526376605,\n",
       "  'token': 3028,\n",
       "  'token_str': ' Daniel',\n",
       "  'sequence': \"Hello my name is Daniel and I'm from India\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=pipeline(\"fill-mask\",top_k=2)\n",
    "classifier(\"Hello my name is <mask> and I'm from India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f3a6f",
   "metadata": {},
   "source": [
    "## NAME ENTITY RECOGNITION(NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b310f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=pipeline(\"ner\",group_entities=True) ## group_entities--> Hugging Face as a single group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe741a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
